{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16cbd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fec5671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from util.ipynb\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from util import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c749a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "class CNN4FL(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 항상 torch.nn.Module을 상속받고 시작\n",
    "        super(CNN4FL, self).__init__()\n",
    "        conv1 = nn.Conv2d(1, 6, 5, 1) # 6@24*24\n",
    "        # activation ReLU\n",
    "        pool1 = nn.MaxPool2d(2) # 6@12*12\n",
    "        conv2 = nn.Conv2d(6, 16, 5, 1) # 16@8*8\n",
    "        # activation ReLU\n",
    "        pool2 = nn.MaxPool2d(2) # 16@4*4\n",
    "        \n",
    "        self.conv_module = nn.Sequential(\n",
    "            conv1,\n",
    "            nn.ReLU(),\n",
    "            pool1,\n",
    "            conv2,\n",
    "            nn.ReLU(),\n",
    "            pool2\n",
    "        )\n",
    "        \n",
    "        fc1 = nn.Linear(16*4*4, 120)\n",
    "        # activation ReLU\n",
    "        fc2 = nn.Linear(120, 84)\n",
    "        # activation ReLU\n",
    "        fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        self.fc_module = nn.Sequential(\n",
    "            fc1,\n",
    "            nn.ReLU(),\n",
    "            fc2,\n",
    "            nn.ReLU(),\n",
    "            fc3\n",
    "        )\n",
    "        \n",
    "        # gpu로 할당\n",
    "        if use_cuda:\n",
    "            self.conv_module = self.conv_module.cuda()\n",
    "            self.fc_module = self.fc_module.cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv_module(x) # @16*4*4\n",
    "        # make linear\n",
    "        dim = 1\n",
    "        for d in out.size()[1:]: #16, 4, 4\n",
    "            dim = dim * d\n",
    "        out = out.view(-1, dim)\n",
    "        out = self.fc_module(out)\n",
    "        return F.softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81399175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        prediction = output.argmax(dim=1, keepdim=True)\n",
    "        correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    return train_loss / len(train_loader), correct/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba098c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += criterion(output, target).item()\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    correct /= len(test_loader.dataset)\n",
    "\n",
    "    return (test_loss, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2d8acc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sync_model(main_model, model_dict):\n",
    "    cnt = len(model_dict)\n",
    "    name_of_models=list(model_dict.keys())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(cnt):\n",
    "            model_dict[name_of_models[i]].load_state_dict(main_model.state_dict())\n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a9d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_local_model(model_dict, criterion_dict, optimizer_dict, \n",
    "                       x_train_dict, y_train_dict, x_test_dict, y_test_dict,\n",
    "                       number_of_samples, epochs, batch_size, verbose=True):\n",
    "    name_of_x_train_sets=list(x_train_dict.keys())\n",
    "    name_of_y_train_sets=list(y_train_dict.keys())\n",
    "    name_of_x_test_sets=list(x_test_dict.keys())\n",
    "    name_of_y_test_sets=list(y_test_dict.keys())\n",
    "\n",
    "    name_of_models=list(model_dict.keys())\n",
    "    name_of_optimizers=list(optimizer_dict.keys())\n",
    "    name_of_criterions=list(criterion_dict.keys())\n",
    "    \n",
    "    logs = list()\n",
    "    if verbose is False:\n",
    "        for i in tqdm(range(number_of_samples), desc='Train local models'):\n",
    "            train_data = DataLoader(TensorDataset(x_train_dict[name_of_x_train_sets[i]],\n",
    "                                                  y_train_dict[name_of_y_train_sets[i]]),\n",
    "                                    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            test_data = DataLoader(TensorDataset(x_test_dict[name_of_x_test_sets[i]],\n",
    "                                                 y_test_dict[name_of_y_test_sets[i]]), batch_size=1)\n",
    "\n",
    "            model = model_dict[name_of_models[i]]\n",
    "            criterion = criterion_dict[name_of_criterions[i]]\n",
    "            optimizer = optimizer_dict[name_of_optimizers[i]]\n",
    "            \n",
    "            epoch_logs = list()\n",
    "            for epoch in range(epochs):\n",
    "                train_loss, train_accuracy = _train(model, train_data, criterion, optimizer)\n",
    "                test_loss, test_accuracy = _evaluate(model, test_data, criterion)\n",
    "                epoch_logs.append([train_loss, train_accuracy, test_loss, test_accuracy])\n",
    "            logs.append(epoch_logs)\n",
    "    else:\n",
    "        for i in range(number_of_samples):    \n",
    "            train_data = DataLoader(TensorDataset(x_train_dict[name_of_x_train_sets[i]],\n",
    "                                                  y_train_dict[name_of_y_train_sets[i]]),\n",
    "                                    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            test_data = DataLoader(TensorDataset(x_test_dict[name_of_x_test_sets[i]],\n",
    "                                                 y_test_dict[name_of_y_test_sets[i]]), batch_size=1)\n",
    "\n",
    "            model = model_dict[name_of_models[i]]\n",
    "            criterion = criterion_dict[name_of_criterions[i]]\n",
    "            optimizer = optimizer_dict[name_of_optimizers[i]]\n",
    "\n",
    "            print('Local_{}'.format(i))\n",
    "            print('--------------------------------------------')\n",
    "            epoch_logs = list()\n",
    "            for epoch in range(epochs):\n",
    "                train_loss, train_accuracy = _train(model, train_data, criterion, optimizer)\n",
    "                test_loss, test_accuracy = _evaluate(model, test_data, criterion)\n",
    "                print(\"[epoch {}/{}]\".format(epoch+1, epochs)\n",
    "                      + \" train_loss: {:0.4f}, train_acc: {:0.4f}\".format(train_loss, train_accuracy)\n",
    "                      + \" | test_loss: {:0.4f}, test_acc: {:0.4f}\".format(test_loss, test_accuracy))\n",
    "                epoch_logs.append([train_loss, train_accuracy, test_loss, test_accuracy])\n",
    "            logs.append(epoch_logs)\n",
    "            print('--------------------------------------------\\n')\n",
    "            \n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02c4c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_main_model(main_model, model_dict):\n",
    "    node_states = list()\n",
    "    node_cnt = len(model_dict)\n",
    "    name_of_models=list(model_dict.keys())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        main_state = main_model.state_dict()\n",
    "    \n",
    "        for key in main_state:\n",
    "            total_state = 0.0\n",
    "            for i in range(node_cnt):\n",
    "                total_state += model_dict[name_of_models[i]].state_dict()[key]\n",
    "            main_state[key] = total_state / float(node_cnt)\n",
    "    \n",
    "    main_model.load_state_dict(main_state)\n",
    "    \n",
    "    return main_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c291f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_local_models(number_of_samples=10, lr=0.01, momentum=0.9):\n",
    "    model_dict = dict()\n",
    "    optimizer_dict = dict()\n",
    "    criterion_dict = dict()\n",
    "    \n",
    "    for i in range(number_of_samples):\n",
    "        model_name = 'model' + str(i)\n",
    "        model_info = CNN4FL()\n",
    "        model_dict.update({model_name: model_info})\n",
    "        \n",
    "        optimizer_name = 'optimizer' + str(i)\n",
    "        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=lr, momentum=momentum)\n",
    "        optimizer_dict.update({optimizer_name: optimizer_info})\n",
    "        \n",
    "        criterion_name = 'criterion' + str(i)\n",
    "        criterion_info = nn.CrossEntropyLoss()\n",
    "        criterion_dict.update({criterion_name: criterion_info})\n",
    "        \n",
    "    return model_dict, optimizer_dict, criterion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b12ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_learning(x_train_dict, y_train_dict, x_test_dict, y_test_dict, x_test, y_test,\n",
    "                       number_of_samples, iteration, epochs, batch_size, log_name, verbose=False):\n",
    "    main_model = CNN4FL()\n",
    "    main_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    local_model_dict, local_optimizer_dict, local_criterion_dict = _create_local_models(number_of_samples)\n",
    "    _, test_data = create_dataloader(None, None, x_test, y_test, batch_size)\n",
    "    \n",
    "    main_logs = list()\n",
    "    local_logs = list()\n",
    "    for i in range(iteration):\n",
    "        print('[*] Iteration: {}/{}'.format(str(i+1), str(iteration)))\n",
    "        local_model_dict = _sync_model(main_model, local_model_dict)\n",
    "        local_log = _train_local_model(local_model_dict, local_criterion_dict, local_optimizer_dict,\n",
    "                                       x_train_dict, y_train_dict, x_test_dict, y_test_dict,\n",
    "                                       number_of_samples, epochs = epochs, batch_size = batch_size,\n",
    "                                       verbose = verbose)\n",
    "        main_model = _update_main_model(main_model, local_model_dict)\n",
    "        test_loss, test_accuracy = _evaluate(main_model, test_data, main_criterion)\n",
    "        print(\"[iter {}/{}]\".format(i+1, iteration)\n",
    "              + \" main_loss: {:0.4f}, main_acc: {:0.4f}\".format(test_loss, test_accuracy))\n",
    "        create_eval_report(main_model, x_test, y_test, printable=verbose)\n",
    "        main_logs.append([test_loss, test_accuracy])\n",
    "        local_logs.append(local_log)\n",
    "    \n",
    "    log_dict = {\n",
    "        'main': main_logs,\n",
    "        'local': local_logs\n",
    "    }\n",
    "\n",
    "    filetime = time.strftime(\"_%Y%m%d-%H%M%S\")\n",
    "    temp_name = '_' + str(number_of_samples) + '_' + str(iteration) + '_' + str(epochs) + '_' + str(batch_size)\n",
    "    filename = './data/exp_result/' + log_name + temp_name + filetime + '.pkl'\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(log_dict, f)\n",
    "    \n",
    "    return main_model, local_model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5dddfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_report(model, x_test, y_test, printable=True):\n",
    "    y_pred = model(x_test)\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "    report = classification_report(y_test, y_pred, digits=4)\n",
    "    if printable:\n",
    "        print(report)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d910836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralized_learning(x_train, y_train, x_test, y_test, epochs, batch_size):\n",
    "    model = CNN4FL()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_data, test_data = create_dataloader(x_train, y_train, x_test, y_test, batch_size)\n",
    "    print(\"------ Centralized Model ------\")\n",
    "    logs = list()\n",
    "    for epoch in range(epochs):\n",
    "        central_train_loss, central_train_accuracy = _train(model, train_data, criterion, optimizer)\n",
    "        central_test_loss, central_test_accuracy = _evaluate(model, test_data, criterion)\n",
    "\n",
    "        print(\"[epoch {}/{}]\".format(epoch+1, epochs)\n",
    "              + \" train loss: {:0.4f}\".format(central_train_loss)\n",
    "              + \", train accuracy: {:7.4f}\".format(central_train_accuracy)\n",
    "              + \" | test loss: {:0.4f}\".format(central_test_loss)\n",
    "              + \", test accuracy: {:7.4f}\".format(central_test_accuracy))\n",
    "        logs.append([central_train_loss, central_train_accuracy, central_test_loss, central_test_accuracy])\n",
    "    print(\"------ Training finished ------\")\n",
    "    \n",
    "    filetime = time.strftime(\"_%Y%m%d-%H%M%S\")\n",
    "    filename = './data/exp_result/' + 'central_' + str(epochs) + '_' + str(batch_size) + filetime + '.pkl'\n",
    "\n",
    "    log_dict = {\n",
    "        'main': logs\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(log_dict, f)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "126616ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_local_and_merged_model(main_model, local_model_dict, x_test_dict, y_test_dict):\n",
    "    number_of_samples = len(local_model_dict)\n",
    "    accuracy_table = pd.DataFrame(data=np.zeros((number_of_samples, 3)),\n",
    "                                  columns=['local', 'local_ind_model', 'merged_main_model'])\n",
    "    for i, (m, x, y) in enumerate(zip(local_model_dict, x_test_dict, y_test_dict)):\n",
    "        local_model = local_model_dict[m]\n",
    "        x_test = x_test_dict[x]\n",
    "        y_test = y_test_dict[y]\n",
    "    \n",
    "        y_pred = local_model(x_test).argmax(dim=1)\n",
    "        local_accuracy = accuracy_score(y_pred, y_test)\n",
    "        \n",
    "        y_pred = main_model(x_test).argmax(dim=1)\n",
    "        main_accuracy = accuracy_score(y_pred, y_test)\n",
    "    \n",
    "        accuracy_table.loc[i, 'local'] = 'local ' + str(i)\n",
    "        accuracy_table.loc[i, 'local_ind_model'] = local_accuracy\n",
    "        accuracy_table.loc[i, 'merged_main_model'] = main_accuracy\n",
    "        \n",
    "    return accuracy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7448dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06671fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    model = CNN4FL()\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bbcbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "federate",
   "language": "python",
   "name": "federate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

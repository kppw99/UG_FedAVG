{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_and_shuffle_labels(y_data, seed):\n",
    "    num_of_class = len(set(y_data.tolist()))\n",
    "    y_data=pd.DataFrame(y_data, columns=['label'])\n",
    "    y_data['index'] = np.arange(len(y_data))\n",
    "    label_dict = dict()\n",
    "    cur_idx = list()\n",
    "\n",
    "    for i in range(num_of_class):\n",
    "        var_name = 'label' + str(i)\n",
    "        label_info = y_data[y_data['label'] == i]\n",
    "        np.random.seed(seed)\n",
    "        label_info = np.random.permutation(label_info)\n",
    "        label_info = pd.DataFrame(label_info, columns=['label', 'index'])\n",
    "        label_dict.update({var_name: label_info })\n",
    "        cur_idx.append(0)\n",
    "\n",
    "    return label_dict, cur_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_iid_subsamples_indices(y_data, number_of_samples, seed):\n",
    "    num_of_class = len(set(y_data.tolist()))\n",
    "    label_dict, cur_idx = _split_and_shuffle_labels(y_data, seed)\n",
    "    sample_dict = dict()\n",
    "    dist = 1.0 / num_of_class\n",
    "    for i in range(number_of_samples):\n",
    "        sample_name = 'sample' + str(i)\n",
    "        dumb = pd.DataFrame()\n",
    "        for j in range(num_of_class):\n",
    "            label_name = str('label') + str(j)\n",
    "            if i == (number_of_samples - 1):\n",
    "                next_idx = len(label_dict[label_name])\n",
    "            else:\n",
    "                next_idx = int(len(label_dict[label_name]) * dist)\n",
    "                next_idx += cur_idx[j]\n",
    "            temp = label_dict[label_name][cur_idx[j]:next_idx]\n",
    "            dumb=pd.concat([dumb, temp], axis=0)\n",
    "            cur_idx[j] = next_idx\n",
    "        dumb.reset_index(drop=True, inplace=True)    \n",
    "        sample_dict.update({sample_name: dumb}) \n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_non_iid_subsamples_indices(y_data, number_of_samples, pdist, seed):    \n",
    "    num_of_class = len(set(y_data.tolist()))\n",
    "    label_dict, cur_idx = _split_and_shuffle_labels(y_data, seed)\n",
    "    sample_dict = dict()\n",
    "    for i in range(number_of_samples):\n",
    "        sample_name = 'sample' + str(i)\n",
    "        dumb = pd.DataFrame()\n",
    "        dist1 = pdist * (2 / 3)\n",
    "        dist2 = pdist - dist1\n",
    "        dist3 = (1.0 - pdist) / (num_of_class - 2)\n",
    "        for j in range(num_of_class):\n",
    "            label_name = str('label') + str(j)\n",
    "            dist = dist1 if j == i else dist2 if (j % 5) == (i % 5) else dist3\n",
    "            if i == (number_of_samples - 1):\n",
    "                next_idx = len(label_dict[label_name])\n",
    "            else:\n",
    "                next_idx = int(len(label_dict[label_name]) * dist)\n",
    "                next_idx += cur_idx[j]\n",
    "            temp = label_dict[label_name][cur_idx[j]:next_idx]\n",
    "            dumb = pd.concat([dumb, temp], axis=0)\n",
    "            cur_idx[j] = next_idx\n",
    "        dumb.reset_index(drop=True, inplace=True)    \n",
    "        sample_dict.update({sample_name: dumb}) \n",
    "    return sample_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_subsamples(sample_dict, x_data, y_data, x_name, y_name):\n",
    "    x_data_dict= dict()\n",
    "    y_data_dict= dict()\n",
    "    \n",
    "    for i in range(len(sample_dict)):  ### len(sample_dict)= number of samples\n",
    "        xname= x_name+str(i)\n",
    "        yname= y_name+str(i)\n",
    "        sample_name=\"sample\"+str(i)\n",
    "        \n",
    "        indices=np.sort(np.array(sample_dict[sample_name]['index']))\n",
    "        \n",
    "        x_info= x_data[indices,:]\n",
    "        x_data_dict.update({xname : x_info})\n",
    "        \n",
    "        y_info= y_data[indices]\n",
    "        y_data_dict.update({yname : y_info})\n",
    "        \n",
    "    return x_data_dict, y_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_corrupted_subsamples(sample_dict, x_data, y_data, x_name, y_name,\n",
    "                                 cor_local_ratio=1.0, cor_label_ratio=0.2, cor_data_ratio=0.5):\n",
    "    x_data_dict= dict()\n",
    "    y_data_dict= dict()\n",
    "    \n",
    "    # make corrupted info\n",
    "    num_of_local = len(sample_dict)\n",
    "    num_of_label = len(set(y_data.tolist()))\n",
    "    cor_local_idx = random.sample(range(0, num_of_local), int(num_of_local * cor_local_ratio))\n",
    "    cor_label_idx = random.sample(range(0, num_of_label), int(num_of_label * cor_label_ratio))\n",
    "    temp = set(y_data.tolist())\n",
    "    temp.difference_update(cor_label_idx)\n",
    "    temp = list(temp)\n",
    "    cor_vals = random.sample(temp, int(num_of_label * cor_label_ratio))\n",
    "    print(cor_label_idx, '->', cor_vals)\n",
    "    \n",
    "    for i in range(len(sample_dict)):  ### len(sample_dict)= number of samples\n",
    "        xname= x_name+str(i)\n",
    "        yname= y_name+str(i)\n",
    "        sample_name=\"sample\"+str(i)\n",
    "        \n",
    "        indices=np.sort(np.array(sample_dict[sample_name]['index']))\n",
    "        \n",
    "        x_info= x_data[indices,:]\n",
    "        x_data_dict.update({xname : x_info})\n",
    "        \n",
    "        y_info= y_data[indices]\n",
    "        \n",
    "        if i in cor_local_idx:\n",
    "            val_cnt = 0\n",
    "            for j in cor_label_idx:\n",
    "                temp_dices = np.where(y_info == j)[0]\n",
    "                cor_data_len = int(len(temp_dices) * cor_data_ratio)\n",
    "                corrupted_idx = random.sample(list(temp_dices), cor_data_len)\n",
    "                \n",
    "                ori_val = y_info[corrupted_idx][0]\n",
    "                cor_val = (ori_val + 1) % num_of_label\n",
    "                y_info[corrupted_idx] = cor_vals[val_cnt]\n",
    "                val_cnt = val_cnt + 1\n",
    "        \n",
    "        y_data_dict.update({yname : y_info})\n",
    "        \n",
    "    return x_data_dict, y_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_dict(x_train_dict, y_train_dict, x_test_dict, y_test_dict):\n",
    "    sum = 0\n",
    "    print('[*] Train Dataset (x, y)')\n",
    "    for idx, (x_key, y_key) in enumerate(zip(x_train_dict, y_train_dict)):\n",
    "        sum += len(x_train_dict[x_key])\n",
    "        print('- sample{}: {}, {}'.format(idx, len(x_train_dict[x_key]), len(y_train_dict[y_key])))\n",
    "        print(': ', end='')\n",
    "        for i in range(10):\n",
    "            print(y_train_dict[y_key].tolist().count(i), end=' ')\n",
    "        print('')\n",
    "    print('# total:', sum, end='\\n\\n')\n",
    "\n",
    "    sum = 0\n",
    "    print('[*] Test Dataset (x, y)')\n",
    "    for idx, (x_key, y_key) in enumerate(zip(x_test_dict, y_test_dict)):\n",
    "        sum += len(x_test_dict[x_key])\n",
    "        print('- sample{}: {}, {}'.format(idx, len(x_test_dict[x_key]), len(y_test_dict[y_key])))\n",
    "        print(': ', end='')\n",
    "        for i in range(10):\n",
    "            print(y_test_dict[y_key].tolist().count(i), end=' ')\n",
    "        print('')\n",
    "    print('# total:', sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data(path='./data/mnist.pkl.gz', torch_tensor=True):\n",
    "    data_path = Path(path)\n",
    "    with gzip.open(data_path, \"rb\") as f:\n",
    "        ((x_train, y_train), (x_test, y_test)) = pickle.load(f)\n",
    "        \n",
    "    if torch_tensor:\n",
    "        x_train, y_train, x_test, y_test = map(torch.tensor, (x_train, y_train, x_test, y_test))\n",
    "    print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_non_iid_samples(x_train, y_train, x_test, y_test, num_of_sample=10, pdist=0.6, seed=1, verbose=True):\n",
    "    sample_dict_train = _get_non_iid_subsamples_indices(y_train, num_of_sample, pdist, seed)\n",
    "    x_train_dict, y_train_dict = _create_subsamples(sample_dict_train, x_train, y_train, 'x_train', 'y_train')\n",
    "    \n",
    "    sample_dict_test = _get_non_iid_subsamples_indices(y_test, num_of_sample, pdist, seed)\n",
    "    x_test_dict, y_test_dict = _create_subsamples(sample_dict_test, x_test, y_test, 'x_test', 'y_test')\n",
    "    \n",
    "    if verbose:\n",
    "        _print_dict(x_train_dict, y_train_dict, x_test_dict, y_test_dict)\n",
    "    return x_train_dict, y_train_dict, x_test_dict, y_test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iid_samples(x_train, y_train, x_test, y_test, num_of_sample=10, seed=1, verbose=True):\n",
    "    sample_dict_train = _get_iid_subsamples_indices(y_train, num_of_sample, seed)\n",
    "    x_train_dict, y_train_dict = _create_subsamples(sample_dict_train, x_train, y_train, 'x_train', 'y_train')\n",
    "    \n",
    "    sample_dict_test = _get_iid_subsamples_indices(y_test, num_of_sample, seed)\n",
    "    x_test_dict, y_test_dict = _create_subsamples(sample_dict_test, x_test, y_test, 'x_test', 'y_test')\n",
    "    \n",
    "    if verbose:\n",
    "        _print_dict(x_train_dict, y_train_dict, x_test_dict, y_test_dict)\n",
    "    return x_train_dict, y_train_dict, x_test_dict, y_test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corrupted_iid_samples(x_train, y_train, x_test, y_test,\n",
    "                                 cor_local_ratio=1.0, cor_label_ratio=0.2, cor_data_ratio=0.5,\n",
    "                                 num_of_sample=10, seed=1, verbose=True):\n",
    "    sample_dict_train = _get_iid_subsamples_indices(y_train, num_of_sample, seed)\n",
    "    x_train_dict, y_train_dict = _create_corrupted_subsamples(sample_dict_train, x_train, y_train,\n",
    "                                                              'x_train', 'y_train',\n",
    "                                                              cor_local_ratio, cor_label_ratio, cor_data_ratio)\n",
    "    \n",
    "    sample_dict_test = _get_iid_subsamples_indices(y_test, num_of_sample, seed)\n",
    "    x_test_dict, y_test_dict = _create_subsamples(sample_dict_test, x_test, y_test, 'x_test', 'y_test')\n",
    "    \n",
    "    if verbose:\n",
    "        _print_dict(x_train_dict, y_train_dict, x_test_dict, y_test_dict)\n",
    "    return x_train_dict, y_train_dict, x_test_dict, y_test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(x_train, y_train, x_test, y_test, batch_size):\n",
    "    train_data = None\n",
    "    test_data = None\n",
    "    \n",
    "    if x_train != None and y_train != None:\n",
    "        train_data = DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    if x_test != None and y_test != None:\n",
    "        test_data = DataLoader(TensorDataset(x_test, y_test), batch_size=1)\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28]) torch.Size([60000]) torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "tr_X, tr_y, te_X, te_y = load_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 6] -> [0, 4]\n",
      "[*] Train Dataset (x, y)\n",
      "- sample0: 5996, 5996\n",
      ": 884 674 595 613 879 542 296 626 293 594 \n",
      "- sample1: 5996, 5996\n",
      ": 884 674 595 613 879 542 296 626 293 594 \n",
      "- sample2: 5996, 5996\n",
      ": 884 674 595 613 879 542 296 626 293 594 \n",
      "- sample3: 5996, 5996\n",
      ": 884 674 595 613 879 542 296 626 293 594 \n",
      "- sample4: 5996, 5996\n",
      ": 884 674 595 613 879 542 296 626 293 594 \n",
      "- sample5: 5996, 5996\n",
      ": 884 674 595 613 879 542 296 626 293 594 \n",
      "- sample6: 5996, 5996\n",
      ": 884 674 595 613 879 542 296 626 293 594 \n",
      "- sample7: 5996, 5996\n",
      ": 884 674 595 613 879 542 296 626 293 594 \n",
      "- sample8: 5996, 5996\n",
      ": 884 674 595 613 879 542 296 626 293 594 \n",
      "- sample9: 6036, 6036\n",
      ": 888 676 603 614 885 543 300 631 293 603 \n",
      "# total: 60000\n",
      "\n",
      "[*] Test Dataset (x, y)\n",
      "- sample0: 996, 996\n",
      ": 98 113 103 101 98 89 95 102 97 100 \n",
      "- sample1: 996, 996\n",
      ": 98 113 103 101 98 89 95 102 97 100 \n",
      "- sample2: 996, 996\n",
      ": 98 113 103 101 98 89 95 102 97 100 \n",
      "- sample3: 996, 996\n",
      ": 98 113 103 101 98 89 95 102 97 100 \n",
      "- sample4: 996, 996\n",
      ": 98 113 103 101 98 89 95 102 97 100 \n",
      "- sample5: 996, 996\n",
      ": 98 113 103 101 98 89 95 102 97 100 \n",
      "- sample6: 996, 996\n",
      ": 98 113 103 101 98 89 95 102 97 100 \n",
      "- sample7: 996, 996\n",
      ": 98 113 103 101 98 89 95 102 97 100 \n",
      "- sample8: 996, 996\n",
      ": 98 113 103 101 98 89 95 102 97 100 \n",
      "- sample9: 1036, 1036\n",
      ": 98 118 105 101 100 91 103 110 101 109 \n",
      "# total: 10000\n"
     ]
    }
   ],
   "source": [
    "tr_X_iid_dict, tr_y_iid_dict, te_X_iid_dict, te_y_iid_dict = create_corrupted_iid_samples(\n",
    "    tr_X, tr_y, te_X, te_y,\n",
    "    cor_local_ratio=1.0, cor_label_ratio=0.2, cor_data_ratio=0.5,\n",
    "    num_of_sample=10, seed=1, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
